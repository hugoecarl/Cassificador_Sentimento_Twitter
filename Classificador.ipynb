{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificador Automático de Sentimento\n",
    "\n",
    "O projeto utiliza o teorema de Bayes para classificar sentimentos baseados em tweets sobre algum determinado assunto ou marca, para esse projeto a marca utilizada foi a Brahma. Foi utilizado o Classificador Naive-Bayes codado de forma manual por ser um trabalho realizado no inicio do curso de engenharia de computação, em 2016. O projeto utiliza a biblioteca tweepy para automaticamente baixar os tweets no Twitter.\n",
    "\n",
    "Sugestão de leitura:<br />\n",
    "http://docs.tweepy.org/en/v3.5.0/index.html<br />\n",
    "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Preparando o ambiente\n",
    "\n",
    "Instalando a biblioteca *tweepy* para realizar a conexão com o Twitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#Instalando o tweepy\n",
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando as Bibliotecas que serão utilizadas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import math\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import json\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Autenticando no  Twitter\n",
    "\n",
    "Para realizar a captura dos dados é necessário ter uma conta cadastrada no twitter:\n",
    "\n",
    "1. Caso ainda não tenha uma: https://twitter.com/signup\n",
    "1. Depois é necessário registrar um app para usar a biblioteca: https://apps.twitter.com/\n",
    "1. Dentro do registro do App, na aba Keys and Access Tokens, anotar os seguintes campos:\n",
    "    1. Consumer Key (API Key)\n",
    "    1. Consumer Secret (API Secret)\n",
    "1. Mais abaixo, gere um Token e anote também:\n",
    "    1. Access Token\n",
    "    1. Access Token Secret\n",
    "    \n",
    "1. Preencha os valores no arquivo \"auth.pass\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'auth.pass'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-795501d46b67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#leitura do arquivo no formato JSON\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'auth.pass'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'auth.pass'"
     ]
    }
   ],
   "source": [
    "#Dados de autenticação do twitter:\n",
    "\n",
    "#leitura do arquivo no formato JSON\n",
    "with open('auth.pass') as fp:    \n",
    "    data = json.load(fp)\n",
    "\n",
    "#Configurando a biblioteca. Não modificar\n",
    "auth = tweepy.OAuthHandler(data['consumer_key'], data['consumer_secret'])\n",
    "auth.set_access_token(data['access_token'], data['access_token_secret'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Coletando Dados\n",
    "\n",
    "Agora vamos coletar os dados. Tenha em mente que dependendo do produto escolhido, não haverá uma quantidade significativa de mensagens, ou ainda poder haver muitos retweets.<br /><br /> \n",
    "Configurando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Produto escolhido:\n",
    "produto = 'Brahma'\n",
    "\n",
    "#Quantidade mínima de mensagens capturadas:\n",
    "n = 500\n",
    "#Quantidade mínima de mensagens para a base de treinamento:\n",
    "t = 300\n",
    "\n",
    "#Filtro de língua, escolha uma na tabela ISO 639-1.\n",
    "lang = 'pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capturando os dados do twitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cria um objeto para a captura\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "#Inicia a captura, para mais detalhes: ver a documentação do tweepy\n",
    "i = 1\n",
    "msgs = []\n",
    "for msg in tweepy.Cursor(api.search, q=produto, lang=lang).items():    \n",
    "    msgs.append(msg.text.lower())\n",
    "    i += 1\n",
    "    if i > n:\n",
    "        break\n",
    "\n",
    "#Embaralhando as mensagens para reduzir um possível viés\n",
    "shuffle(msgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando os dados em uma planilha Excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Verifica se o arquivo não existe para não substituir um conjunto pronto\n",
    "if not os.path.isfile('./{0}.xlsx'.format(produto)):\n",
    "    \n",
    "    #Abre o arquivo para escrita\n",
    "    writer = pd.ExcelWriter('{0}.xlsx'.format(produto))\n",
    "\n",
    "    #divide o conjunto de mensagens em duas planilhas\n",
    "    dft = pd.DataFrame({'Treinamento' : pd.Series(msgs[:t])})\n",
    "    dft.to_excel(excel_writer = writer, sheet_name = 'Treinamento', index = False)\n",
    "\n",
    "    dfc = pd.DataFrame({'Teste' : pd.Series(msgs[t:])})\n",
    "    dfc.to_excel(excel_writer = writer, sheet_name = 'Teste', index = False)\n",
    "\n",
    "    #fecha o arquivo\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Classificando as Mensagens\n",
    "\n",
    "Agora você deve abrir o arquivo Excel com as mensagens capturadas e classificar na Coluna B se a mensagem é relevante ou não.<br /> \n",
    "\n",
    "___\n",
    "## Montando o Classificador Naive-Bayes\n",
    "\n",
    "\n",
    "* Limpando as mensagens removendo os caracteres: enter, :, \", ', (, ), etc. Não remover emojis.<br />\n",
    "* Corrigindo separação de espaços entre palavras e/ou emojis.\n",
    "* Realizando outras limpezas/transformações que não afetem a qualidade da informação.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_excel('Brahma.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevantes = dados[dados.Classific == 'R'] \n",
    "irrelevantes = dados[dados.Classific == 'I']\n",
    "listaR = [] \n",
    "LR = [] \n",
    "listaI = [] \n",
    "LI = [] \n",
    "LT = []\n",
    "listaT = []\n",
    "excluir=[\"-\",\"!\",\"_\",\";\",\"+\",\"@\",\"/\",\"]\",\".\", \",\",\"=\",\"'\",\"?\",\"%\",\"#\",\"*\",\":\",\"\\,\"] \n",
    "\n",
    "for i in relevantes.Treinamento: \n",
    "    separa=i.split()\n",
    "    for j in separa:\n",
    "        letras=list(j)\n",
    "        for l in letras:\n",
    "            if l in excluir:\n",
    "                letras.remove(l)\n",
    "        palavra=''.join(letras)\n",
    "        listaR.append(palavra)\n",
    "        \n",
    "for i in irrelevantes.Treinamento: \n",
    "    separa1=i.split()\n",
    "    for j in separa1:\n",
    "        letras1=list(j)\n",
    "        for l1 in letras1:\n",
    "            if l1 in excluir:\n",
    "                letras1.remove(l1)\n",
    "        palavra1=''.join(letras1)\n",
    "        listaI.append(palavra1) \n",
    "        \n",
    "for i in dados.Treinamento: \n",
    "    separa2=i.split()\n",
    "    for j in separa2:\n",
    "        letras2=list(j)\n",
    "        for l2 in letras2:\n",
    "            if l2 in excluir:\n",
    "                letras2.remove(l2)\n",
    "        palavra2=''.join(letras2)\n",
    "        listaT.append(palavra2)    \n",
    "\n",
    "\n",
    "for h in listaR: \n",
    "    if h not in LR:\n",
    "        LR.append(h)\n",
    "\n",
    "for h in listaI:  \n",
    "    if h not in LI:\n",
    "        LI.append(h)\n",
    "\n",
    "for h in listaT:  \n",
    "    if h not in LT:\n",
    "        LT.append(h)\n",
    "        \n",
    "        \n",
    "LT=list(set(LT))\n",
    "NumR = len(LR)\n",
    "NumI= len(LI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Verificando a performance\n",
    "\n",
    "\n",
    "* Porcentagem de positivos falsos (marcados como relevante mas não são relevantes)\n",
    "* Porcentagem de positivos verdadeiros (marcado como relevante e são relevantes)\n",
    "* Porcentagem de negativos verdadeiros (marcado como não relevante e não são relevantes)\n",
    "* Porcentagem de negativos falsos (marcado como não relevante e são relevantes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dadosteste=pd.read_excel(\"Brahma.xlsx\",sheetname=\"Teste\")\n",
    "\n",
    "PF=0\n",
    "PV=0\n",
    "NF=0\n",
    "NV=0\n",
    "muito_relevante=0 \n",
    "relevante=0\n",
    "neutro=0\n",
    "irrelevante=0\n",
    "muito_irrelevante=0\n",
    "\n",
    "ProbabilidadeR=[]\n",
    "Relevantes=[1]\n",
    "ProbabilidadeI=[]\n",
    "Irrelevantes=[1]\n",
    "ListaRe=[]\n",
    "ListaIR=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(dadosteste.Teste)): \n",
    "    ProbabilidadeR.append([]) \n",
    "    ProbabilidadeI.append([]) \n",
    "    for j in range(len(dadosteste.Teste[i].split())): \n",
    "        conta1=LR.count(dadosteste.Teste[i].split()[j]) \n",
    "        conta2=LI.count(dadosteste.Teste[i].split()[j])\n",
    "        \n",
    "        ProbabilidadeR[i].append((conta1+1)/(NumR+len(LT))) \n",
    "        ProbabilidadeI[i].append((conta2+1)/(NumI+len(LT)))      \n",
    "        \n",
    "        Relevantes[i]=(Relevantes[i]*ProbabilidadeR[i][j]) \n",
    "        Irrelevantes[i]=(Irrelevantes[i]*ProbabilidadeI[i][j])\n",
    "        \n",
    "    Relevantes.append(1)\n",
    "    Irrelevantes.append(1)       \n",
    "    \n",
    "for i in range(len(dadosteste[\"Classific\"])):\n",
    "   \n",
    "    if Relevantes[i]<Irrelevantes[i]:\n",
    "        \n",
    "        ListaRe.append(Relevantes[i])\n",
    "        if dadosteste.Classific[i]==\"R\":\n",
    "            \n",
    "            PV+=1\n",
    "        else:\n",
    "            PF+=1                 \n",
    "            \n",
    "    else:\n",
    "        ListaIR.append(Irrelevantes[i])\n",
    "        if dadosteste.Classific[i]==\"I\":            \n",
    "            NV+=1\n",
    "            \n",
    "        else:            \n",
    "            NF+=1\n",
    "ListaRe.sort()\n",
    "ListaIR.sort()\n",
    "\n",
    "vcentral_relevancia = ListaRe[int(len(ListaRe)/2)]\n",
    "vcentral_irrelevancia =  ListaIR[int(len(ListaRe)/2)]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(ListaRe)):\n",
    "    if Relevantes[i]>=ListaRe[int(len(ListaRe)*0.75)]:\n",
    "        muito_relevante+=1\n",
    "    else:\n",
    "        if Relevantes[i]>=ListaRe[int(len(ListaRe)*0.25)]:\n",
    "            relevante+=1\n",
    "        else:\n",
    "            neutro+=1\n",
    "\n",
    "for i in range(len(ListaIR)):\n",
    "    if Irrelevantes[i]>=ListaIR[int(len(ListaIR)*0.75)]:\n",
    "        muito_irrelevante+=1\n",
    "    else:\n",
    "        if Irrelevantes[i]>=ListaIR[int(len(ListaIR)*0.25)]:\n",
    "            irrelevante+=1\n",
    "        else:\n",
    "            neutro+=1\n",
    "\n",
    "\n",
    "probabilidadeFP=PF/dadosteste[\"Teste\"].shape[0]\n",
    "probabilidadeVP=PV/dadosteste[\"Teste\"].shape[0]\n",
    "probabilidadeFN=NF/dadosteste[\"Teste\"].shape[0]\n",
    "probabilidadeVN=NV/dadosteste[\"Teste\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positivos Verdadeiros :1.00%\n",
      "Negativos Verdadeiros: 69.00%\n",
      "Positivos Falsos: 2.50%\n",
      "Negativos Falsos : 27.50%\n"
     ]
    }
   ],
   "source": [
    "print(\"Positivos Verdadeiros :{0:.2f}%\".format(probabilidadeVP*100))\n",
    "print(\"Negativos Verdadeiros: {0:.2f}%\".format(probabilidadeVN*100))\n",
    "print(\"Positivos Falsos: {0:.2f}%\".format(probabilidadeFP*100))\n",
    "print(\"Negativos Falsos : {0:.2f}%\".format(probabilidadeFN*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muito Relevantes :0.00%\n",
      "Relevantes: 2.00%\n",
      "Neutros: 12.50%\n",
      "Irrelevantes : 60.00%\n",
      "Muito Irrelevantes : 25.50%\n"
     ]
    }
   ],
   "source": [
    "print(\"Muito Relevantes :{0:.2f}%\".format((muito_relevante/len(dadosteste[\"Classific\"]))*100))\n",
    "print(\"Relevantes: {0:.2f}%\".format((relevante/len(dadosteste[\"Classific\"]))*100))\n",
    "print(\"Neutros: {0:.2f}%\".format((neutro/len(dadosteste[\"Classific\"]))*100))\n",
    "print(\"Irrelevantes : {0:.2f}%\".format((irrelevante/len(dadosteste[\"Classific\"]))*100))   \n",
    "print(\"Muito Irrelevantes : {0:.2f}%\".format((muito_irrelevante/len(dadosteste[\"Classific\"]))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de Acertos :70.00%\n",
      "Total de Erros  : 30.00%\n"
     ]
    }
   ],
   "source": [
    "print(\"Total de Acertos :{0:.2f}%\".format(probabilidadeVP*100+probabilidadeVN*100))\n",
    "print(\"Total de Erros  : {0:.2f}%\".format(probabilidadeFP*100+probabilidadeFN*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "___\n",
    "## Concluindo\n",
    "\n",
    "\n",
    "    \n",
    " Para montar o algoritimo nos baseamos no classificador Naive-Bayes que classifica as mensagens em Relevantes ou Irrelevantes seguindo o conceito de machine learning. Analisando palavra por palavra usando o conceito de probabilidade, podemos discorrer que o classificador é preciso, porém não consegue analisar casos de dupla negação, sarcasmo ou alguma forma de exprimir sentimentos por meio de um significado implícito no contexto da frase. No caso do produto analisado por ser uma marca de cerveja, um produto popular, barato e associado a um entorpecimento mental, podemos observar muitos tweets relacionados a promoções, pessoas informando o consumo do produto, sentimentos demonstrados por meio de emoticons, frases inacabadas e opiniões implicitas que dificultam quando a classificação é palavra por palavra. A seguir podemos analisar 5 exemplos de tweets que o método de analisar palavra por palavra não funciona:\n",
    " \n",
    " *  só uma menina pra beber uma brahma comigo amanhã ao som de art popular...\n",
    " *  nunca vi uma terça com tanta cara de sexta, na moral só queria eu e meus amigos num bar tomando um choppzinho da brahma bem gelado\n",
    " *  como q alguem bebe brahma meu deus\n",
    " *  dia ta pedindo uma praia e umas brahma\n",
    " *  brahma é brahma né!? 😍\n",
    " \n",
    " Percebemos com isso que o classificador acaba perdendo muita qualidade e até se confunde às vezes, entretanto por ser muito maleável seria ideal para analisar outros produtos ou outras redes sociais onde o publico alvo escreva de forma mais clara e opinativa. Segundo a taxa alta (69%) de negativos verdadeiros percebemos que o algoritmo entende bem a não relevância alta dos tweets seguindo a classificação humana, e como podemos enxergar abaixo a grande maioria Neutros, Irrelevantes e Muito Irrelevantes. \n",
    " \n",
    "- Muito Relevantes :0.00%\n",
    "- Relevantes: 2.00%\n",
    "- Neutros: 12.50%\n",
    "- Irrelevantes : 60.00%\n",
    "- Muito Irrelevantes : 25.50%\n",
    "\n",
    " No caso da empresa Brahma analisando as mensagens no twitter para realmente haver uma plano de expanção e um aumento na eficácia do comparador seria necessario um código mais complexo capaz de analisar frases ao invés de apenas palavras, implementando uma forma de analisar emoticons e até figuras de linguagem como metáforas e onomatopeias por exemplo. Retirando todos os tweets repetidos também. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificador Autom√°tico de Sentimento\n",
    "\n",
    "O projeto utiliza o teorema de Bayes para classificar sentimentos baseados em tweets sobre algum determinado assunto ou marca, para esse projeto a marca utilizada foi a Brahma. Foi utilizado o Classificador Naive-Bayes codado de forma manual por ser um trabalho realizado no inicio do curso de engenharia de computa√ß√£o, em 2016. O projeto utiliza a biblioteca tweepy para automaticamente baixar os tweets no Twitter.\n",
    "\n",
    "Sugest√£o de leitura:<br />\n",
    "http://docs.tweepy.org/en/v3.5.0/index.html<br />\n",
    "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Preparando o ambiente\n",
    "\n",
    "Instalando a biblioteca *tweepy* para realizar a conex√£o com o Twitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#Instalando o tweepy\n",
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando as Bibliotecas que ser√£o utilizadas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import math\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import json\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Autenticando no  Twitter\n",
    "\n",
    "Para realizar a captura dos dados √© necess√°rio ter uma conta cadastrada no twitter:\n",
    "\n",
    "1. Caso ainda n√£o tenha uma: https://twitter.com/signup\n",
    "1. Depois √© necess√°rio registrar um app para usar a biblioteca: https://apps.twitter.com/\n",
    "1. Dentro do registro do App, na aba Keys and Access Tokens, anotar os seguintes campos:\n",
    "    1. Consumer Key (API Key)\n",
    "    1. Consumer Secret (API Secret)\n",
    "1. Mais abaixo, gere um Token e anote tamb√©m:\n",
    "    1. Access Token\n",
    "    1. Access Token Secret\n",
    "    \n",
    "1. Preencha os valores no arquivo \"auth.pass\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'auth.pass'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-795501d46b67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#leitura do arquivo no formato JSON\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'auth.pass'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'auth.pass'"
     ]
    }
   ],
   "source": [
    "#Dados de autentica√ß√£o do twitter:\n",
    "\n",
    "#leitura do arquivo no formato JSON\n",
    "with open('auth.pass') as fp:    \n",
    "    data = json.load(fp)\n",
    "\n",
    "#Configurando a biblioteca. N√£o modificar\n",
    "auth = tweepy.OAuthHandler(data['consumer_key'], data['consumer_secret'])\n",
    "auth.set_access_token(data['access_token'], data['access_token_secret'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Coletando Dados\n",
    "\n",
    "Agora vamos coletar os dados. Tenha em mente que dependendo do produto escolhido, n√£o haver√° uma quantidade significativa de mensagens, ou ainda poder haver muitos retweets.<br /><br /> \n",
    "Configurando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Produto escolhido:\n",
    "produto = 'Brahma'\n",
    "\n",
    "#Quantidade m√≠nima de mensagens capturadas:\n",
    "n = 500\n",
    "#Quantidade m√≠nima de mensagens para a base de treinamento:\n",
    "t = 300\n",
    "\n",
    "#Filtro de l√≠ngua, escolha uma na tabela ISO 639-1.\n",
    "lang = 'pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capturando os dados do twitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cria um objeto para a captura\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "#Inicia a captura, para mais detalhes: ver a documenta√ß√£o do tweepy\n",
    "i = 1\n",
    "msgs = []\n",
    "for msg in tweepy.Cursor(api.search, q=produto, lang=lang).items():    \n",
    "    msgs.append(msg.text.lower())\n",
    "    i += 1\n",
    "    if i > n:\n",
    "        break\n",
    "\n",
    "#Embaralhando as mensagens para reduzir um poss√≠vel vi√©s\n",
    "shuffle(msgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando os dados em uma planilha Excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Verifica se o arquivo n√£o existe para n√£o substituir um conjunto pronto\n",
    "if not os.path.isfile('./{0}.xlsx'.format(produto)):\n",
    "    \n",
    "    #Abre o arquivo para escrita\n",
    "    writer = pd.ExcelWriter('{0}.xlsx'.format(produto))\n",
    "\n",
    "    #divide o conjunto de mensagens em duas planilhas\n",
    "    dft = pd.DataFrame({'Treinamento' : pd.Series(msgs[:t])})\n",
    "    dft.to_excel(excel_writer = writer, sheet_name = 'Treinamento', index = False)\n",
    "\n",
    "    dfc = pd.DataFrame({'Teste' : pd.Series(msgs[t:])})\n",
    "    dfc.to_excel(excel_writer = writer, sheet_name = 'Teste', index = False)\n",
    "\n",
    "    #fecha o arquivo\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Classificando as Mensagens\n",
    "\n",
    "Agora voc√™ deve abrir o arquivo Excel com as mensagens capturadas e classificar na Coluna B se a mensagem √© relevante ou n√£o.<br /> \n",
    "\n",
    "___\n",
    "## Montando o Classificador Naive-Bayes\n",
    "\n",
    "\n",
    "* Limpando as mensagens removendo os caracteres: enter, :, \", ', (, ), etc. N√£o remover emojis.<br />\n",
    "* Corrigindo separa√ß√£o de espa√ßos entre palavras e/ou emojis.\n",
    "* Realizando outras limpezas/transforma√ß√µes que n√£o afetem a qualidade da informa√ß√£o.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_excel('Brahma.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevantes = dados[dados.Classific == 'R'] \n",
    "irrelevantes = dados[dados.Classific == 'I']\n",
    "listaR = [] \n",
    "LR = [] \n",
    "listaI = [] \n",
    "LI = [] \n",
    "LT = []\n",
    "listaT = []\n",
    "excluir=[\"-\",\"!\",\"_\",\";\",\"+\",\"@\",\"/\",\"]\",\".\", \",\",\"=\",\"'\",\"?\",\"%\",\"#\",\"*\",\":\",\"\\,\"] \n",
    "\n",
    "for i in relevantes.Treinamento: \n",
    "    separa=i.split()\n",
    "    for j in separa:\n",
    "        letras=list(j)\n",
    "        for l in letras:\n",
    "            if l in excluir:\n",
    "                letras.remove(l)\n",
    "        palavra=''.join(letras)\n",
    "        listaR.append(palavra)\n",
    "        \n",
    "for i in irrelevantes.Treinamento: \n",
    "    separa1=i.split()\n",
    "    for j in separa1:\n",
    "        letras1=list(j)\n",
    "        for l1 in letras1:\n",
    "            if l1 in excluir:\n",
    "                letras1.remove(l1)\n",
    "        palavra1=''.join(letras1)\n",
    "        listaI.append(palavra1) \n",
    "        \n",
    "for i in dados.Treinamento: \n",
    "    separa2=i.split()\n",
    "    for j in separa2:\n",
    "        letras2=list(j)\n",
    "        for l2 in letras2:\n",
    "            if l2 in excluir:\n",
    "                letras2.remove(l2)\n",
    "        palavra2=''.join(letras2)\n",
    "        listaT.append(palavra2)    \n",
    "\n",
    "\n",
    "for h in listaR: \n",
    "    if h not in LR:\n",
    "        LR.append(h)\n",
    "\n",
    "for h in listaI:  \n",
    "    if h not in LI:\n",
    "        LI.append(h)\n",
    "\n",
    "for h in listaT:  \n",
    "    if h not in LT:\n",
    "        LT.append(h)\n",
    "        \n",
    "        \n",
    "LT=list(set(LT))\n",
    "NumR = len(LR)\n",
    "NumI= len(LI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Verificando a performance\n",
    "\n",
    "\n",
    "* Porcentagem de positivos falsos (marcados como relevante mas n√£o s√£o relevantes)\n",
    "* Porcentagem de positivos verdadeiros (marcado como relevante e s√£o relevantes)\n",
    "* Porcentagem de negativos verdadeiros (marcado como n√£o relevante e n√£o s√£o relevantes)\n",
    "* Porcentagem de negativos falsos (marcado como n√£o relevante e s√£o relevantes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dadosteste=pd.read_excel(\"Brahma.xlsx\",sheetname=\"Teste\")\n",
    "\n",
    "PF=0\n",
    "PV=0\n",
    "NF=0\n",
    "NV=0\n",
    "muito_relevante=0 \n",
    "relevante=0\n",
    "neutro=0\n",
    "irrelevante=0\n",
    "muito_irrelevante=0\n",
    "\n",
    "ProbabilidadeR=[]\n",
    "Relevantes=[1]\n",
    "ProbabilidadeI=[]\n",
    "Irrelevantes=[1]\n",
    "ListaRe=[]\n",
    "ListaIR=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(dadosteste.Teste)): \n",
    "    ProbabilidadeR.append([]) \n",
    "    ProbabilidadeI.append([]) \n",
    "    for j in range(len(dadosteste.Teste[i].split())): \n",
    "        conta1=LR.count(dadosteste.Teste[i].split()[j]) \n",
    "        conta2=LI.count(dadosteste.Teste[i].split()[j])\n",
    "        \n",
    "        ProbabilidadeR[i].append((conta1+1)/(NumR+len(LT))) \n",
    "        ProbabilidadeI[i].append((conta2+1)/(NumI+len(LT)))      \n",
    "        \n",
    "        Relevantes[i]=(Relevantes[i]*ProbabilidadeR[i][j]) \n",
    "        Irrelevantes[i]=(Irrelevantes[i]*ProbabilidadeI[i][j])\n",
    "        \n",
    "    Relevantes.append(1)\n",
    "    Irrelevantes.append(1)       \n",
    "    \n",
    "for i in range(len(dadosteste[\"Classific\"])):\n",
    "   \n",
    "    if Relevantes[i]<Irrelevantes[i]:\n",
    "        \n",
    "        ListaRe.append(Relevantes[i])\n",
    "        if dadosteste.Classific[i]==\"R\":\n",
    "            \n",
    "            PV+=1\n",
    "        else:\n",
    "            PF+=1                 \n",
    "            \n",
    "    else:\n",
    "        ListaIR.append(Irrelevantes[i])\n",
    "        if dadosteste.Classific[i]==\"I\":            \n",
    "            NV+=1\n",
    "            \n",
    "        else:            \n",
    "            NF+=1\n",
    "ListaRe.sort()\n",
    "ListaIR.sort()\n",
    "\n",
    "vcentral_relevancia = ListaRe[int(len(ListaRe)/2)]\n",
    "vcentral_irrelevancia =  ListaIR[int(len(ListaRe)/2)]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(ListaRe)):\n",
    "    if Relevantes[i]>=ListaRe[int(len(ListaRe)*0.75)]:\n",
    "        muito_relevante+=1\n",
    "    else:\n",
    "        if Relevantes[i]>=ListaRe[int(len(ListaRe)*0.25)]:\n",
    "            relevante+=1\n",
    "        else:\n",
    "            neutro+=1\n",
    "\n",
    "for i in range(len(ListaIR)):\n",
    "    if Irrelevantes[i]>=ListaIR[int(len(ListaIR)*0.75)]:\n",
    "        muito_irrelevante+=1\n",
    "    else:\n",
    "        if Irrelevantes[i]>=ListaIR[int(len(ListaIR)*0.25)]:\n",
    "            irrelevante+=1\n",
    "        else:\n",
    "            neutro+=1\n",
    "\n",
    "\n",
    "probabilidadeFP=PF/dadosteste[\"Teste\"].shape[0]\n",
    "probabilidadeVP=PV/dadosteste[\"Teste\"].shape[0]\n",
    "probabilidadeFN=NF/dadosteste[\"Teste\"].shape[0]\n",
    "probabilidadeVN=NV/dadosteste[\"Teste\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positivos Verdadeiros :1.00%\n",
      "Negativos Verdadeiros: 69.00%\n",
      "Positivos Falsos: 2.50%\n",
      "Negativos Falsos : 27.50%\n"
     ]
    }
   ],
   "source": [
    "print(\"Positivos Verdadeiros :{0:.2f}%\".format(probabilidadeVP*100))\n",
    "print(\"Negativos Verdadeiros: {0:.2f}%\".format(probabilidadeVN*100))\n",
    "print(\"Positivos Falsos: {0:.2f}%\".format(probabilidadeFP*100))\n",
    "print(\"Negativos Falsos : {0:.2f}%\".format(probabilidadeFN*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muito Relevantes :0.00%\n",
      "Relevantes: 2.00%\n",
      "Neutros: 12.50%\n",
      "Irrelevantes : 60.00%\n",
      "Muito Irrelevantes : 25.50%\n"
     ]
    }
   ],
   "source": [
    "print(\"Muito Relevantes :{0:.2f}%\".format((muito_relevante/len(dadosteste[\"Classific\"]))*100))\n",
    "print(\"Relevantes: {0:.2f}%\".format((relevante/len(dadosteste[\"Classific\"]))*100))\n",
    "print(\"Neutros: {0:.2f}%\".format((neutro/len(dadosteste[\"Classific\"]))*100))\n",
    "print(\"Irrelevantes : {0:.2f}%\".format((irrelevante/len(dadosteste[\"Classific\"]))*100))   \n",
    "print(\"Muito Irrelevantes : {0:.2f}%\".format((muito_irrelevante/len(dadosteste[\"Classific\"]))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de Acertos :70.00%\n",
      "Total de Erros  : 30.00%\n"
     ]
    }
   ],
   "source": [
    "print(\"Total de Acertos :{0:.2f}%\".format(probabilidadeVP*100+probabilidadeVN*100))\n",
    "print(\"Total de Erros  : {0:.2f}%\".format(probabilidadeFP*100+probabilidadeFN*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "___\n",
    "## Concluindo\n",
    "\n",
    "\n",
    "    \n",
    " Para montar o algoritimo nos baseamos no classificador Naive-Bayes que classifica as mensagens em Relevantes ou Irrelevantes seguindo o conceito de machine learning. Analisando palavra por palavra usando o conceito de probabilidade, podemos discorrer que o classificador √© preciso, por√©m n√£o consegue analisar casos de dupla nega√ß√£o, sarcasmo ou alguma forma de exprimir sentimentos por meio de um significado impl√≠cito no contexto da frase. No caso do produto analisado por ser uma marca de cerveja, um produto popular, barato e associado a um entorpecimento mental, podemos observar muitos tweets relacionados a promo√ß√µes, pessoas informando o consumo do produto, sentimentos demonstrados por meio de emoticons, frases inacabadas e opini√µes implicitas que dificultam quando a classifica√ß√£o √© palavra por palavra. A seguir podemos analisar 5 exemplos de tweets que o m√©todo de analisar palavra por palavra n√£o funciona:\n",
    " \n",
    " *  s√≥ uma menina pra beber uma brahma comigo amanh√£ ao som de art popular...\n",
    " *  nunca vi uma ter√ßa com tanta cara de sexta, na moral s√≥ queria eu e meus amigos num bar tomando um choppzinho da brahma bem gelado\n",
    " *  como q alguem bebe brahma meu deus\n",
    " *  dia ta pedindo uma praia e umas brahma\n",
    " *  brahma √© brahma n√©!? üòç\n",
    " \n",
    " Percebemos com isso que o classificador acaba perdendo muita qualidade e at√© se confunde √†s vezes, entretanto por ser muito male√°vel seria ideal para analisar outros produtos ou outras redes sociais onde o publico alvo escreva de forma mais clara e opinativa. Segundo a taxa alta (69%) de negativos verdadeiros percebemos que o algoritmo entende bem a n√£o relev√¢ncia alta dos tweets seguindo a classifica√ß√£o humana, e como podemos enxergar abaixo a grande maioria Neutros, Irrelevantes e Muito Irrelevantes. \n",
    " \n",
    "- Muito Relevantes :0.00%\n",
    "- Relevantes: 2.00%\n",
    "- Neutros: 12.50%\n",
    "- Irrelevantes : 60.00%\n",
    "- Muito Irrelevantes : 25.50%\n",
    "\n",
    " No caso da empresa Brahma analisando as mensagens no twitter para realmente haver uma plano de expan√ß√£o e um aumento na efic√°cia do comparador seria necessario um c√≥digo mais complexo capaz de analisar frases ao inv√©s de apenas palavras, implementando uma forma de analisar emoticons e at√© figuras de linguagem como met√°foras e onomatopeias por exemplo. Retirando todos os tweets repetidos tamb√©m. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
